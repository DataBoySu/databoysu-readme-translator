"""
Translation module for the README translator action.
Handles chunking, translation via LLM, and navbar injection.
"""
import os
import re
import argparse

# translation pipeline for GitHub Action

LANG_MAP = {
    "de": "German", "fr": "French", "es": "Spanish", "ja": "Japanese",
    "zh": "Chinese(Simplified)", 
    "ru": "Russian", "pt": "Portuguese", "ko": "Korean", "hi": "Hindi",
    "ar": "Arabic", "cs": "Czech", "nl": "Dutch", "en": "English",
    "el": "Greek", "he": "Hebrew", "id": "Indonesian", "it": "Italian",
    "fa": "Persian", "pl": "Polish", "ro": "Romanian", "tr": "Turkish",
    "uk": "Ukrainian", "vi": "Vietnamese", "zh-tw": "Chinese(Traditional)",
}

NAV_DATA = {
    "ar": ("ğŸ‡¸ğŸ‡¦", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"),
    "cs": ("ğŸ‡¨ğŸ‡¿", "ÄŒeÅ¡tina"),
    "de": ("ğŸ‡©ğŸ‡ª", "Deutsch"),
    "el": ("ğŸ‡¬ğŸ‡·", "Î•Î»Î»Î·Î½Î¹ÎºÎ¬"),
    "en": ("ğŸ‡ºğŸ‡¸", "English"),
    "es": ("ğŸ‡ªğŸ‡¸", "EspaÃ±ol"),
    "fa": ("ğŸ‡®ğŸ‡·", "ÙØ§Ø±Ø³ÛŒ"),
    "fr": ("ğŸ‡«ğŸ‡·", "FranÃ§ais"),
    "he": ("ğŸ‡®ğŸ‡±", "×¢×‘×¨×™×ª"),
    "hi": ("ğŸ‡®ğŸ‡³", "à¤¹à¤¿à¤‚à¤¦à¥€"),
    "id": ("ğŸ‡®ğŸ‡©", "Bahasa Indonesia"),
    "it": ("ğŸ‡®ğŸ‡¹", "Italiano"),
    "ja": ("ğŸ‡¯ğŸ‡µ", "æ—¥æœ¬èª"),
    "ko": ("ğŸ‡°ğŸ‡·", "í•œêµ­ì–´"),
    "nl": ("ğŸ‡³ğŸ‡±", "Nederlands"),
    "pl": ("ğŸ‡µğŸ‡±", "Polski"),
    "pt": ("ğŸ‡µğŸ‡¹", "PortuguÃªs"),
    "ro": ("ğŸ‡·ğŸ‡´", "RomÃ¢nÄƒ"),
    "ru": ("ğŸ‡·ğŸ‡º", "Ğ ÑƒÑÑĞºĞ¸Ğ¹"),
    "tr": ("ğŸ‡¹ğŸ‡·", "TÃ¼rkÃ§e"),
    "uk": ("ğŸ‡ºğŸ‡¦", "Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°"),
    "vi": ("ğŸ‡»ğŸ‡³", "Tiáº¿ng Viá»‡t"),
    "zh": ("ğŸ‡¨ğŸ‡³", "ä¸­æ–‡"),
    "zh-tw": ("ğŸ‡¹ğŸ‡¼", "ç¹é«”ä¸­æ–‡"),
}

# Forbidden phrases that indicate hallucination
FORBIDDEN = [
    # English
    "This section", "In this", "In this section", "means", "explains",
    # Chinese (Simplified)
    "ä»¥ä¸‹", "è¯´æ˜", "æœ¬èŠ‚", "åœ¨è¿™é‡Œ", "æ„å‘³ç€", "è§£é‡Š",
    # German
    "Dieser Abschnitt", "In diesem", "In diesem Abschnitt", "bedeutet", "erklÃ¤rt",
    # French
    "Cette section", "Dans cette", "Dans cette section", "signifie", "explique",
    # Spanish
    "Esta secciÃ³n", "En esta", "En esta secciÃ³n", "significa", "explica",
    # Japanese
    "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³", "ã“ã®ä¸­ã§", "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯", "æ„å‘³ã™ã‚‹", "èª¬æ˜ã™ã‚‹",
    # Russian
    "Ğ­Ñ‚Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»", "Ğ’ ÑÑ‚Ğ¾Ğ¼", "Ğ’ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğµ", "Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚", "Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚", "Ğ½Ğ¸Ğ¶Ğµ",
    # Arabic
    "Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…", "ÙÙŠ Ù‡Ø°Ø§", "ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…", "ÙŠØ¹Ù†ÙŠ", "ÙŠØ´Ø±Ø­",
    # Czech
    "Tato sekce", "V tomto", "V tÃ©to sekci", "znamenÃ¡", "vysvÄ›tluje",
    # Dutch
    "Deze sectie", "In dit", "In deze sectie", "betekent", "verklaart",
    # Greek
    "Î‘Ï…Ï„ÏŒ Ï„Î¿ Ï„Î¼Î®Î¼Î±", "Î£Îµ Î±Ï…Ï„ÏŒ", "Î£Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ Ï„Î¼Î®Î¼Î±", "ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹", "ÎµÎ¾Î·Î³ÎµÎ¯",
    # Hebrew
    "×¡×¢×™×£ ×–×”", "×‘×–×”", "×‘×¡×¢×™×£ ×–×”", "××©××¢×•×ª×•", "××¡×‘×™×¨",
    # Indonesian
    "Bagian ini", "Dalam ini", "Di bagian ini", "berarti", "menjelaskan",
    # Italian
    "Questa sezione", "In questo", "In questa sezione", "significa", "spiega",
    # Persian (Farsi)
    "Ø§ÛŒÙ† Ø¨Ø®Ø´", "Ø¯Ø± Ø§ÛŒÙ†", "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´", "Ù…Ø¹Ù†ÛŒ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯", "ØªÙˆØ¶ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯",
    # Polish
    "Ta sekcja", "W tym", "W tej sekcji", "oznacza", "wyjaÅ›nia",
    # Romanian
    "AceastÄƒ secÈ›iune", "Ãn acest", "Ãn aceastÄƒ secÈ›iune", "Ã®nseamnÄƒ", "explicÄƒ",
    # Turkish
    "Bu bÃ¶lÃ¼m", "Bunda", "Bu bÃ¶lÃ¼mde", "anlamÄ±na gelir", "aÃ§Ä±klar",
    # Ukrainian
    "Ğ¦ĞµĞ¹ Ñ€Ğ¾Ğ·Ğ´Ñ–Ğ»", "Ğ£ Ñ†ÑŒĞ¾Ğ¼Ñƒ", "Ğ£ Ñ†ÑŒĞ¾Ğ¼Ñƒ Ñ€Ğ¾Ğ·Ğ´Ñ–Ğ»Ñ–", "Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ”", "Ğ¿Ğ¾ÑÑĞ½ÑÑ”",
    # Vietnamese
    "Pháº§n nÃ y", "Trong nÃ y", "Trong pháº§n nÃ y", "cÃ³ nghÄ©a lÃ ", "giáº£i thÃ­ch",
    # Traditional Chinese
    "ä»¥ä¸‹", "èªªæ˜", "æœ¬ç¯€", "åœ¨é€™è£¡", "æ„å‘³è‘—", "è§£é‡‹",
    # Portuguese
    "Esta seÃ§Ã£o", "Nesta seÃ§Ã£o", "significa", "explica",
    # Korean
    "ì´ ì„¹ì…˜", "ì´ ì•ˆì—ì„œ", "ì´ ì„¹ì…˜ì—ì„œëŠ”", "ì˜ë¯¸í•œë‹¤", "ì„¤ëª…í•œë‹¤",
    # Hindi
    "à¤¯à¤¹ à¤…à¤¨à¥à¤­à¤¾à¤—", "à¤‡à¤¸à¤®à¥‡à¤‚", "à¤‡à¤¸ à¤…à¤¨à¥à¤­à¤¾à¤— à¤®à¥‡à¤‚", "à¤•à¤¾ à¤…à¤°à¥à¤¥ à¤¹à¥ˆ", "à¤¸à¤®à¤à¤¾à¤¤à¤¾ à¤¹à¥ˆ",
]

# Language-specific expansion multipliers for length validation
HIGH_MULTIPLIER_MAP = {
    "ja": 5.5,  # Japanese can expand significantly
    "hi": 5.5,  # Hindi often requires more tokens
    "ar": 4.0,  # Arabic expands moderately
    "he": 4.0,  # Hebrew
    "fa": 4.0,  # Persian (Farsi)
    "ru": 3.5,  # Russian
    "uk": 3.5,  # Ukrainian
    "pl": 3.5,  # Polish
}

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# 2. Smart Chunking Functions

def _classify_text_as_struct_or_prose(text):
    """Classify text chunk as structure (HTML/comments) or prose."""
    t = text.strip()
    if (
        t.startswith(('<div', '<details', '```')) or
        t.startswith('<!--') or t.endswith('-->') or
        # Strict check: Chunk must be ONLY images/links (no prose text)
        re.fullmatch(r'(?:\s*(?:!\[.*?\]\(.*?\)|\[.*?\]\(.*?\))\s*)+', t, flags=re.DOTALL)
    ):
        return 'struct'
    return 'prose'


def split_struct_blockquotes(chunks):
    """Split any `struct` chunk that contains a markdown blockquote into
    a `struct` part before the quote, a `prose` blockquote part, and an
    optional tail (struct/prose) after. This handles cases where placeholders
    like <!-- HTML_BLOCK --> are adjacent to a quoted one-line description.
    """
    out = []
    for ctype, ctext in chunks:
        if ctype != 'struct' or not re.search(r'^\s*>', ctext, flags=re.MULTILINE):
            out.append((ctype, ctext))
            continue

        # Work with original lines to preserve spacing
        lines = ctext.splitlines(True)

        # find first line that starts with '>' (block quote)
        start = None
        for idx, line in enumerate(lines):
            if line.lstrip().startswith('>'):
                start = idx
                break

        if start is None:
            out.append((ctype, ctext))
            continue

        # find end of contiguous blockquote region, include adjacent blank lines
        end = start
        while end + 1 < len(lines):
            nxt = lines[end + 1]
            if nxt.lstrip().startswith('>'):
                end += 1
                continue
            # include a single blank line immediately after blockquote
            if nxt.strip() == '':
                # only include if followed by another blockquote line
                if end + 2 < len(lines) and lines[end + 2].lstrip().startswith('>'):
                    end += 1
                    continue
                # otherwise, treat blank as separator and stop
                break
            break

        before = ''.join(lines[:start]).strip()
        block = ''.join(lines[start:end+1]).strip()
        after = ''.join(lines[end+1:]).strip()

        if before:
            out.append(('struct', before))
        out.append(('prose', block))
        if after:
            out.append((_classify_text_as_struct_or_prose(after), after))

    return out

def get_smart_chunks(text):
    """Split text into smart chunks based on markdown/html patterns."""
    pattern = r'(' \
              r'```[\s\S]*?```|' \
              r'<div\b[^>]*>[\s\S]*?<\/div>|' \
              r'<details\b[^>]*>[\s\S]*?<\/details>|' \
              r'<section\b[^>]*>[\s\S]*?<\/section>|' \
              r'<table\b[^>]*>[\s\S]*?<\/table>|' \
              r'^\s*(?:[!\[].*?\]\(.*?\)\s*)+$|' \
              r'^#{1,6} .*' \
              r')'

    raw_parts = re.split(pattern, text, flags=re.MULTILINE | re.IGNORECASE)
    chunks = []

    for part in raw_parts:
        if not part or not part.strip():
            continue
        
        stripped_part = part.strip()

        # Treat blockquotes as prose
        if stripped_part.startswith('>'):
            chunks.append(("prose", stripped_part))
            continue

        if (
            stripped_part.startswith(('<div', '<details', '<section', '<table', '```')) or
            stripped_part.startswith('<!--') or stripped_part.endswith('-->') or
            re.match(r'!\[.*?\]\(.*?\)', stripped_part) or
            re.match(r'\[.*?\]\(.*?\)', stripped_part)
        ):
            chunks.append(("struct", stripped_part))
        else:
            chunks.append(("prose", stripped_part))

    return chunks


def merge_small_chunks(chunks, min_chars=50):
    """Merge small prose chunks to prevent fragmentation."""
    merged = []
    i = 0
    while i < len(chunks):
        ctype, ctext = chunks[i]

        # Check if chunk is too small or is a header, and merge with next if possible
        is_small = len(ctext) < min_chars
        if ctype == "prose" and (ctext.startswith('#') or is_small) and i + 1 < len(chunks):
            next_ctype, next_ctext = chunks[i+1]
            combined_text = ctext + "\n\n" + next_ctext
            new_type = "hybrid" if next_ctype == "struct" else "prose"
            merged.append((new_type, combined_text))
            i += 2
        else:
            merged.append((ctype, ctext))
            i += 1
    return merged


# 3. Prompts
def translate_chunk(text, llm, prompts, lang_guidance=None, is_lone_header=False):
    """Translate a single chunk using llama-cpp-python."""
    current_system_prompt = prompts['header'] if is_lone_header else prompts['prose']
    if lang_guidance and not is_lone_header:
        current_system_prompt = f"{prompts['prose']}\n\nADDITIONAL GUIDANCE:\n{lang_guidance}"

    prompt = (
        f"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\n{current_system_prompt}\n<|END_OF_TURN_TOKEN|>\n"
        f"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>\n{text}<|END_OF_TURN_TOKEN|>\n"
        "<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>"
    )

    # Dynamic max_tokens to prevent infinite loops on small inputs
    # Estimate: 3 tokens per char upper bound, min 256, max 4096
    estimated_limit = int(len(text) * 3) + 200
    gen_limit = min(4096, max(256, estimated_limit))

    response = llm(prompt, max_tokens=gen_limit, temperature=0, stop=["<|END_OF_TURN_TOKEN|>"])
    translated = response['choices'][0]['text'].strip()

    if translated.startswith("```"):
        lines = translated.splitlines()
        if lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].strip().startswith("```"):
            lines = lines[:-1]
        translated = "\n".join(lines).strip()

    return translated


def inject_navbar(readme_text, langs):
    """Inject or update the navigation bar in the README."""
    start_marker = '<!--START_SECTION:navbar-->'
    end_marker = '<!--END_SECTION:navbar-->'

    links = []
    # Always include English (Root) first
    flag, name = NAV_DATA.get("en", ("ğŸ‡ºğŸ‡¸", "English"))
    links.append(f'<a href="README.md">{flag} {name}</a>')

    for l in sorted(langs):
        if l == "en": continue
        if l in NAV_DATA:
            flag, name = NAV_DATA[l]
        else:
            flag, name = "ğŸ³ï¸", l.upper()
        href = f"locales/README.{l}.md"
        links.append(f'<a href="{href}">{flag} {name}</a>')

    navbar_content = ' | '.join(links)
    html_block = f'<div align="center">\n  {navbar_content}\n</div>'
    block = f"{start_marker}\n{html_block}\n{end_marker}\n\n"

    # Regex to replace existing block (handling potential multiline content between markers)
    pattern = re.compile(f'{re.escape(start_marker)}.*?{re.escape(end_marker)}\s*', re.DOTALL)

    if pattern.search(readme_text):
        return pattern.sub(block, readme_text)
    else:
        return block + readme_text


def get_system_prompts(target_lang_name):
    """Generate system prompts for the target language."""
    header = (
        f"You are a technical translation filter for {target_lang_name}.\n"
        "STRICT RULES:\n"
        "- The input is a single section header. Translate it 1:1.\n"
        "- DO NOT generate any content, lists, or descriptions under the header.\n"
        "- Preserve the '#' symbols exactly.\n"
        "- Output ONLY the translated header.\n"
        "- Preserve original formatting, punctuation, whitespace, and markdown/code symbols exactly;"
        " do NOT normalize, reflow, or 'fix' the input."
    )

    prose = (
        f"You are a professional technical translation engine. "
        f"Your task: Translate the input into {target_lang_name}.\n"
        "STRICT RULES:\n"
        "- Output ONLY the final translated text. No intros.\n"
        "- NEVER modify HTML tags, attributes (href, src), or CSS styles.\n"
        "- Keep technical terms in English.\n"
        "- Preserve all Markdown symbols (#, **, `, -, link) exactly.\n"
        "- Do NOT translate GitHub Flavored Markdown alerts (e.g., '> [!NOTE]', '> [!IMPORTANT]').\n"
        "- Do NOT translate badge/shield alt text or URLs.\n"
        "- Do NOT modify formatting, whitespace, punctuation, code fences, list markers, "
        "or emphasis markers; translate only the human-visible text.\n"
        "- Markdown Admonitions: NEVER translate the keyword inside > [!KEYWORD]. Valid keywords are: NOTE, TIP, IMPORTANT, WARNING, CAUTION.\n"
        "- Static Badges: Do not translate text inside image URLs (e.g., img.shields.io) unless it is the alt text.\n"
        "- Emoji Integrity: Ensure emojis remain attached to their correct logical counterparts."

    )
    return header, prose


def load_guidance(lang):
    """Load language-specific guidance from scripts directory."""
    scripts_dir = os.path.join(BASE_DIR, "scripts")
    guidance_file = os.path.join(scripts_dir, f"{lang}.txt")
    if os.path.exists(guidance_file):
        with open(guidance_file, "r", encoding="utf-8") as f:
            return f.read().strip()
    return ""


def process_chunks(chunks, llm, lang, prompts, lang_guidance):
    """Translate chunks and return joined text."""
    final_output = []
    multiplier = HIGH_MULTIPLIER_MAP.get(lang, 2.5)

    total_chunks = len(chunks)
    print(f"[INFO] Processing {total_chunks} chunks for language '{lang}'...", flush=True)

    for i, (ctype, ctext) in enumerate(chunks):
        if ctype == 'struct':
            final_output.append(ctext + '\n\n')
            continue

        print(f"[INFO] Translating chunk {i+1}/{total_chunks} ({len(ctext)} chars)...", flush=True)
        is_lone_header = ctext.strip().startswith('#') and '\n' not in ctext.strip()
        translated = translate_chunk(
            ctext, llm, prompts, lang_guidance, is_lone_header
        )

        if len(translated) > multiplier * len(ctext) or any(f in translated for f in FORBIDDEN):
            print(f"[WARN] Chunk {i+1} failed validation, using original text", flush=True)
            translated = ctext

        final_output.append(translated.rstrip() + '\n\n')

    return ''.join(final_output)


def run_translation_pipeline(content, llm, lang, prompts, lang_guidance):
    """Execute the chunking and translation steps."""
    chunks = get_smart_chunks(content)
    chunks = split_struct_blockquotes(chunks)
    chunks = merge_small_chunks(chunks)

    full_text = process_chunks(chunks, llm, lang, prompts, lang_guidance)
    
    # Post-processing: Fix relative paths
    full_text = re.sub(r'(\[.*?\]\()(?!(?:http|/|#|\.\./))', r'\1../', full_text)
    full_text = re.sub(r'((?:src|href)=["\'])(?!(?:http|/|#|\.\./))', r'\1../', full_text)
    
    return full_text


def regenerate_all_navbars(readme_path, locales_dir):
    """Regenerate navbars for the root README and all locale files."""
    if not os.path.exists(locales_dir):
        print(f"[INFO] No locales directory found at {locales_dir}. Skipping navbar generation.")
        return

    # Discover languages
    langs = []
    for f in os.listdir(locales_dir):
        match = re.match(r'README\.(.+?)\.md$', f)
        if match and match.group(1) in NAV_DATA:
            langs.append(match.group(1))
    langs.sort()
    
    # Helper to generate HTML
    def get_nav_html(is_root):
        links = []
        # English (Root)
        en_flag, en_name = NAV_DATA.get('en', ('ğŸ‡ºğŸ‡¸', 'English'))
        en_href = 'README.md' if is_root else '../README.md'
        links.append(f'<a href="{en_href}">{en_flag} {en_name}</a>')
        
        for l in langs:
            flag, name = NAV_DATA.get(l, ('ğŸ³ï¸', l.upper()))
            href = f'locales/README.{l}.md' if is_root else f'README.{l}.md'
            links.append(f'<a href="{href}">{flag} {name}</a>')
        
        return ' | '.join(links)

    # Helper to update file
    def update_file(path, block):
        if not os.path.exists(path): return
        with open(path, 'r', encoding='utf-8') as f: content = f.read()
        start, end = '<!--START_SECTION:navbar-->', '<!--END_SECTION:navbar-->'
        # Regex to replace existing block
        pattern = re.compile(f'{re.escape(start)}.*?{re.escape(end)}\s*', re.DOTALL)
        if pattern.search(content):
            content = pattern.sub(block, content)
        else:
            content = block + content
        with open(path, 'w', encoding='utf-8') as f: f.write(content)

    # 1. Update Root
    root_nav = get_nav_html(is_root=True)
    start, end = '<!--START_SECTION:navbar-->', '<!--END_SECTION:navbar-->'
    root_block = f'{start}\n<div align="center">\n  {root_nav}\n</div>\n{end}\n\n'
    update_file(readme_path, root_block)

    # 2. Update Locales
    locale_nav = get_nav_html(is_root=False)
    locale_block = f'{start}\n<div align="center">\n  {locale_nav}\n</div>\n{end}\n\n'
    for l in langs:
        update_file(os.path.join(locales_dir, f'README.{l}.md'), locale_block)
    
    print(f"[SUCCESS] Regenerated navbars for Root and {len(langs)} locales.")


def update_navbar_in_readme(readme_path, output_dir, lang):
    """Discover locales and update the README navbar."""
    locales_dir = output_dir
    discovered = []
    if os.path.isdir(locales_dir):
        for fname in os.listdir(locales_dir):
            m = re.match(r'README\.(.+?)\.md$', fname)
            if m:
                discovered.append(m.group(1))

    # Ensure current language is present
    if lang not in discovered:
        discovered.append(lang)

    # sort for deterministic order, but keep existing order if present
    # we'll sort to keep behavior predictable
    locales = sorted(discovered)

    with open(readme_path, 'r', encoding='utf-8') as f:
        original = f.read()

    updated = inject_navbar(original, locales)

    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(updated)


def main(lang, model_path='', nav_target='README.md', mode='translate'):
    """Run translation for a single language.

    Parameters:
    - lang: language code
    - model_path: path to GGUF model file
    - nav_target: README path relative to repo root
    - mode: 'translate' or 'navbar'
    """
    # Use current working directory for target repo files
    readme_path = os.path.abspath(nav_target)
    output_dir = os.path.join(os.getcwd(), "locales")

    if mode == 'navbar':
        regenerate_all_navbars(readme_path, output_dir)
        return

    target_lang_name = LANG_MAP.get(lang, "English")

    header_prompt, prose_prompt = get_system_prompts(target_lang_name)
    prompts = {'header': header_prompt, 'prose': prose_prompt}
    lang_guidance = load_guidance(lang)

    output_path = os.path.join(output_dir, f"README.{lang}.md")

    # Initialize LLM here to avoid import-time side-effects
    # pylint: disable=line-too-long
    # pylint: disable=import-error
    from llama_cpp import Llama
    mp = model_path or os.path.join(BASE_DIR, 'models', 'aya-expanse-8b-Q4_K_M.gguf')
    llm = Llama(model_path=mp, n_ctx=8192, n_threads=2, verbose=False)

    os.makedirs(output_dir, exist_ok=True)

    with open(readme_path, 'r', encoding='utf-8') as f:
        content = f.read()

    translated_text = run_translation_pipeline(content, llm, lang, prompts, lang_guidance)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(translated_text)

    update_navbar_in_readme(readme_path, output_dir, lang)
    
    print(f'[SUCCESS] Wrote translated locales to {output_path} and injected navbar into {readme_path}')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--lang", type=str, default="")
    parser.add_argument("--model-path", type=str, default="")
    parser.add_argument("--nav-target", type=str, default="README.md")
    parser.add_argument("--mode", type=str, default="translate")
    args = parser.parse_args()

    if args.mode == "translate" and not args.lang:
        parser.error("the following arguments are required: --lang")

    main(args.lang, model_path=args.model_path, nav_target=args.nav_target, mode=args.mode)
